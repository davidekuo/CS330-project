# Few-shot Classification of Drug Target Activity augmented with pretrained protein embeddings
![GitHub](https://img.shields.io/badge/CS330-Final%20Project-red)

## Contributors
David Kuo, Tom Pritsky, Davey Huang

## Summary
A key component of the drug discovery process is hit identification and lead optimization, which involves identifying compounds that bind to a biological target of interest and optimizing target affinity. Due to the paucity and heterogeneity of high-quality experimental data, we propose a novel few-shot meta-learning methodology that efficiently brings together chemical structure information and biological protein information, leveraging recent advances in large language models for chemical and biological sequence data.

We created a few-shot protein-ligand binding classification dataset of 1047 target protein tasks from 17818 protein-ligand affinity measurements in the **BindingDB** dataset and leveraged the pre-trained embeddings from **ChemBERTa** and **ESM-2** transformer models to represent each small molecule ligand and target protein sequence, respectively. We additionally trained a variational autoencoder (VAE) to reduce the dimensionality of the generated ligand and protein embeddings for improved computational efficiency.

We implemented and evaluated 3 different meta-learning models on 5-shot 2-way classification of binding affinity: an LSTM-based black-box meta-learner, a BERT-based black-box meta-learner, and a non-parametric prototypical network. For each of our model classes, we experimented with using only SMILES embeddings as inputs, concatenating SMILES and protein embeddings as inputs, and concatenating SMILES and protein embeddings with reduced dimensionality (d = 50) generated by a variational autoencoder (VAE) as inputs. For the black-box meta-learners, we additionally explored introducing protein embeddings at an intermediate step rather than immediately prior to input to the model, in order to assess the optimal combination point.

Our best performing model was the LSTM black-box meta-learning with protein embeddings concatenated at the beginning of the model, which attained a test set query accuracy of 70.7%. For all our models, we found that introducing target protein information, in particular at the beginning of the model's forward pass, led to the greatest improvement in few-shot performance, and that dimensionality reduction with a variational autoencoder maintained comparable accuracy while significantly improving runtime.

Full details and results can be found in our [paper](https://drive.google.com/file/d/1d1Ib2igC_w4fXp0aSPkD0oMxM_9Rux7A/view?usp=sharing) and [poster](https://drive.google.com/file/d/1FuvUvIklHkr54paQFeIgf3Mnz3V8LSNF/view?usp=sharing)

## Set Up
1) Clone this repository
2) Inside the repository, create a new virtual environment and activate it, for example, `python3 -m venv venv` and `source venv/bin/activate`
3) Run `pip install -r requirements.txt` to install the required dependencies

## Code
- `bert.py`: code for BERT-based black-box meta-learner
- `lstm.py`: code for LSTM-based black-box meta-learner
- `protonet.py`: code for prototypical network meta-learner
- `data_loader.py`: code for dataloader used for both BERT and LSTM models
- `evaluation.ipynb`: IPython Notebook for evaluating a trained BERT or LSTM model
- `precompute_smiles_embeddings.ipynb`: IPython Notebook for precomputing and storing SMILES embeddings

## Data / Files
- `/data/`: training, validation, and test datasets stored as JSON files
- `/embeddings/`: precomputed protein embeddings stored as JSON and pickle files
