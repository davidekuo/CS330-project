{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data_loader import DataGenerator\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig\n",
    "from bert_utils import SmilesBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Generator\n",
    "train_iterable = DataGenerator(\n",
    "    data_json_path=f'data/test.json',\n",
    "    k=5,\n",
    "    repr=\"smiles_only\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.zeros((1,2))\n",
    "l = torch.zeros((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "l[0,1] = 1\n",
    "pred[0,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(l).sum().item() / (\n",
    "    1 * 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.]]) tensor([[0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(pred, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5032)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn(pred, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "k = 5\n",
    "batch_size = 3\n",
    "\n",
    "# Create Data Generator\n",
    "train_iterable = DataGenerator(\n",
    "    data_json_path=f'data/train.json',\n",
    "    k=k,\n",
    "    repr=\"smiles_only\",\n",
    ")\n",
    "train_loader = iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        train_iterable,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "repr_to_input_dims = {\"smiles_only\": 2 + 767, \n",
    "                        \"concat\": 2 + 767 + 640,\n",
    "                        \"concat_smiles_vaeprot\": 2 + 767 + 100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)):\n\u001b[1;32m     24\u001b[0m     i, l \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(train_loader)\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[39m=\u001b[39m model(i\u001b[39m.\u001b[39;49mfloat(), l\u001b[39m.\u001b[39;49mfloat(), attention_mask)\n\u001b[1;32m     26\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m     optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/anaconda/envs/cs330/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/CS_330_final/bert_utils.py:88\u001b[0m, in \u001b[0;36mSmilesBertModel.forward\u001b[0;34m(self, input_images, input_labels, attention_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(query_embeddings)\n\u001b[1;32m     87\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 88\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m()\n\u001b[1;32m     90\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     91\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, query_labels)\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_config = BertConfig(\n",
    "    max_position_embeddings = (k+1)*2,\n",
    "    hidden_size = 2 + 767,\n",
    "    num_hidden_layers = 4,\n",
    "    num_attention_heads = 1,\n",
    "    intermediate_size = 64,\n",
    "    classifier_dropout = 0.3,\n",
    "    attention_probs_dropout_prob = 0.3,\n",
    "    hidden_dropout_prob = 0.3,\n",
    "    k = 5,\n",
    "    batch_size = 3,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = SmilesBertModel(model_config)\n",
    "# model.to(device)\n",
    "\n",
    "# Create optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "attention_mask = torch.ones((batch_size, (k+1)*2))\n",
    "\n",
    "for step in tqdm(range(100)):\n",
    "    i, l = next(train_loader)\n",
    "    loss = model(i.float(), l.float(), attention_mask)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if (step+1) % 10 == 0: \n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cs330')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ff75cf23da80adcc9f356df2f104d0dd849597226f7b6e4b651ec67f115766"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
