{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data_loader import DataGenerator\n",
    "from tqdm import tqdm\n",
    "from transformers import BertConfig\n",
    "from bert_utils import SmilesBertModel\n",
    "\n",
    "k = 5\n",
    "batch_size = 3\n",
    "\n",
    "# Create Data Generator\n",
    "train_iterable = DataGenerator(\n",
    "    data_json_path=f'data/train.json',\n",
    "    k=k,\n",
    "    repr=\"smiles_only\",\n",
    ")\n",
    "train_loader = iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        train_iterable,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "repr_to_input_dims = {\"smiles_only\": 2 + 767, \n",
    "                        \"concat\": 2 + 767 + 640,\n",
    "                        \"concat_smiles_vaeprot\": 2 + 767 + 100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:00<00:02, 31.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1557, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:00<00:02, 29.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0523, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:01<00:02, 31.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0232, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:01<00:01, 32.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0153, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:01<00:01, 33.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0101, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:02<00:01, 33.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0097, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [00:02<00:00, 32.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0066, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [00:02<00:00, 32.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0062, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [00:02<00:00, 33.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0048, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 31.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0050, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = BertConfig(\n",
    "    max_position_embeddings = (k+1)*2,\n",
    "    hidden_size = 2 + 767,\n",
    "    num_hidden_layers = 4,\n",
    "    num_attention_heads = 1,\n",
    "    intermediate_size = 64,\n",
    "    classifier_dropout = 0.3,\n",
    "    attention_probs_dropout_prob = 0.3,\n",
    "    hidden_dropout_prob = 0.3,\n",
    "    k = 5,\n",
    "    batch_size = 3,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = SmilesBertModel(model_config)\n",
    "# model.to(device)\n",
    "\n",
    "# Create optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "attention_mask = torch.ones((batch_size, (k+1)*2))\n",
    "\n",
    "for step in tqdm(range(100)):\n",
    "    i, l = next(train_loader)\n",
    "    loss = model(i.float(), l.float(), attention_mask)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if (step+1) % 10 == 0: \n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(input_images, input_labels):\n",
    "    # 1. Zero out query set labels\n",
    "    input_labels = input_labels.copy() # .clone()\n",
    "    input_labels[:, -1, :, :] = 0\n",
    "\n",
    "    # 2. Concatenate labels to images\n",
    "    input_images_and_labels = torch.cat((input_images, input_labels), -1)\n",
    "\n",
    "    # 3. Reshape\n",
    "    B, K_1, N, D = input_images_and_labels.shape\n",
    "    input_images_and_labels = input_images_and_labels.reshape((B, -1, D))\n",
    "    return input_images_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_iterable:\n",
    "    smiles, labels = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles[-1, :, :] = 0\n",
    "smiles_and_labels = np.concatenate((smiles, labels), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch, seq_len, hidden_dim) -> (batch, seq_len, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from transformers import BertConfig\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any\n",
    "from utils import MoleculeDataset, SmilesBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(config: Dict[str, Any]):\n",
    "    # Load training set\n",
    "    train_dataset = MoleculeDataset(config[\"train_path\"])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    # Load validation set\n",
    "    val_dataset = MoleculeDataset(config[\"val_path\"])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])\n",
    "\n",
    "    # Check if model was provided, in the case of loading the pretrained model\n",
    "    if \"load_model_path\" in config:\n",
    "        model = torch.load(config[\"load_model_path\"])\n",
    "    else:\n",
    "        # Load vocab\n",
    "        with open(\"preprocessed/vocab.pickle\", \"rb\") as f:\n",
    "            vocab = pickle.load(f)\n",
    "        \n",
    "        # Set model configurations\n",
    "        model_config = BertConfig(\n",
    "            pad_token_id = vocab[\"PAD\"],\n",
    "            vocab_size = len(vocab),\n",
    "            max_position_embeddings = 128,\n",
    "            hidden_size = 64,\n",
    "            num_hidden_layers = 4,\n",
    "            num_attention_heads = 4,\n",
    "            intermediate_size = 64,\n",
    "            classifier_dropout = config[\"classifier_dropout\"],\n",
    "            attention_probs_dropout_prob = config[\"attention_probs_dropout_prob\"],\n",
    "            hidden_dropout_prob = config[\"hidden_dropout_prob\"],\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = SmilesBertModel(model_config)\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    running_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        loss, _ = model(**batch)\n",
    "        running_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Log training loss\n",
    "    print(f\"epoch {epoch}, train MSE loss {running_train_loss / len(train_dataloader)}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        # Compute average MSE loss\n",
    "        for batch in val_dataloader:\n",
    "            loss, _ = model(**batch)\n",
    "            val_loss += loss\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            torch.save(model, config[\"model_save_path\"])\n",
    "        print(f\"epoch {epoch}, val MSE loss {avg_val_loss}\")\n",
    "    model.train()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cs330')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "411970a4c113e84c486338039ec980b217258c316ac659742fac33b33a5986ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
