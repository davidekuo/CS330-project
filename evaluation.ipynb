{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/cs330/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data_loader import DataGenerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hw1_copy import MANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "meta_batch_size = 128\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmilesBertModel(\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (key): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (value): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=769, out_features=64, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=64, out_features=769, bias=True)\n",
       "          (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (key): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (value): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=769, out_features=64, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=64, out_features=769, bias=True)\n",
       "          (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (key): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (value): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=769, out_features=64, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=64, out_features=769, bias=True)\n",
       "          (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (key): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (value): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=769, out_features=769, bias=True)\n",
       "            (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=769, out_features=64, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=64, out_features=769, bias=True)\n",
       "          (LayerNorm): LayerNorm((769,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=769, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iterable = DataGenerator(\n",
    "    data_json_path=f'data/test.json',\n",
    "    k=k,\n",
    "    repr=\"smiles_only\",\n",
    ")\n",
    "test_loader = iter(\n",
    "    torch.utils.data.DataLoader(\n",
    "        test_iterable,\n",
    "        batch_size=meta_batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "model = torch.load(\"model/bert_model.pt\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m i, l \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(test_loader)\n\u001b[1;32m      6\u001b[0m i, l \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mto(device), l\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m pred \u001b[39m=\u001b[39m model(i, l)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m      8\u001b[0m \u001b[39m# pred ~ (B, K+1, N, N)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# model sees K support examples for each of N classes and predicts on 1 query example for each of N classes (all shuffled ofc) -> (_, K+1, N, _)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# here, LSTM outputs logits for each of N classes for each of the (K+1) * N support & query examples -> (_, K+1, N, N)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# batch to leverage parallelism -> (B, K+1, N, N)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m     14\u001b[0m     pred,\n\u001b[1;32m     15\u001b[0m     [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     ],\n\u001b[1;32m     21\u001b[0m )  \u001b[39m# no change, already in correct shape\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/cs330/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "### LSTM\n",
    "num_correct = 0\n",
    "N = 1000\n",
    "for _ in tqdm(range(N)):\n",
    "    i, l = next(test_loader)\n",
    "    i, l = i.to(device), l.to(device)\n",
    "    pred = model(i, l).detach()\n",
    "    # pred ~ (B, K+1, N, N)\n",
    "    # model sees K support examples for each of N classes and predicts on 1 query example for each of N classes (all shuffled ofc) -> (_, K+1, N, _)\n",
    "    # here, LSTM outputs logits for each of N classes for each of the (K+1) * N support & query examples -> (_, K+1, N, N)\n",
    "    # batch to leverage parallelism -> (B, K+1, N, N)\n",
    "\n",
    "    pred = torch.reshape(\n",
    "        pred,\n",
    "        [\n",
    "            -1,\n",
    "            k + 1,\n",
    "            num_classes,\n",
    "            num_classes,\n",
    "        ],\n",
    "    )  # no change, already in correct shape\n",
    "    pred_class = torch.argmax(pred[:, -1, :, :], axis=2)\n",
    "    # pred[:, -1, :, :] selects logits for query example (not K support examples) over entire batch, shape ~ (B, N, N)\n",
    "    # torch.argmax(..., axis=2) selects predicted class (with largest logit) for the N query examples (1 for each class), shape ~ (B, N)\n",
    "    true_class = torch.argmax(l[:, -1, :, :], axis=2)  # selects ground-truth class for the N query examples\n",
    "    num_correct += pred_class.eq(true_class).sum().item()  # sums the number of matches between predicted and ground-truth class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:20<00:00, 49.68it/s]\n"
     ]
    }
   ],
   "source": [
    "### BERT\n",
    "num_correct = 0\n",
    "N = 1000\n",
    "for _ in tqdm(range(N)):\n",
    "    i, l = next(test_loader)\n",
    "    i, l = i.to(device), l.to(device)\n",
    "    attention_mask = torch.ones((meta_batch_size, (k+1)*num_classes))\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    _, pred = model(i.float(), l.float(), attention_mask)\n",
    "    # pred ~ (B, K+1, N, N)\n",
    "    # model sees K support examples for each of N classes and predicts on 1 query example for each of N classes (all shuffled ofc) -> (_, K+1, N, _)\n",
    "    # here, LSTM outputs logits for each of N classes for each of the (K+1) * N support & query examples -> (_, K+1, N, N)\n",
    "    # batch to leverage parallelism -> (B, K+1, N, N)\n",
    "\n",
    "    pred_class = torch.argmax(pred, axis=1)\n",
    "    # pred[:, -1, :, :] selects logits for query example (not K support examples) over entire batch, shape ~ (B, N, N)\n",
    "    # torch.argmax(..., axis=2) selects predicted class (with largest logit) for the N query examples (1 for each class), shape ~ (B, N)\n",
    "    true_class = torch.argmax(l[:, -1, :, :], axis=2)  # selects ground-truth class for the N query examples\n",
    "    num_correct += pred_class.eq(true_class).sum().item()  # sums the number of matches between predicted and ground-truth class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.653078125\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy\", num_correct / (meta_batch_size * num_classes * N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = torchmetrics.functional.classification.binary_roc(preds=pred[:, -1, :, :], target=l[:, -1, :, :])\n",
    "auc = torchmetrics.functional.classification.binary_auroc(preds=pred[:, -1, :, :], target=l[:, -1, :, :]).item()\n",
    "roc_curve = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc)\n",
    "roc_curve.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cs330')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ff75cf23da80adcc9f356df2f104d0dd849597226f7b6e4b651ec67f115766"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
